{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEUHhZknbL5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sichun1247/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from math import ceil, floor, log\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import sys\n",
    "\n",
    "# from keras_applications.resnet import ResNet50\n",
    "from keras_applications.inception_v3 import InceptionV3\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "test_images_dir = '/home/sichun1247/Sampled_Test/'\n",
    "train_images_dir = '/home/sichun1247/Sampled_Train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jp_zYTWNbbc9"
   },
   "outputs": [],
   "source": [
    "def read_testset(filename=\"/home/sichun1247/test_sample.csv\"):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    \n",
    "    df = df.loc[:, [\"Label\", \"Type\", \"ID_Image\"]]\n",
    "    df = df.set_index(['ID_Image', 'Type']).unstack(level=-1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_trainset(filename=\"/home/sichun1247/train_sample.csv\"):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    \n",
    "    \n",
    "    df = df.loc[:, [\"Label\", \"Type\", \"ID_Image\"]]\n",
    "    df = df.set_index(['ID_Image', 'Type']).unstack(level=-1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = read_trainset()   \n",
    "test_df = read_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4K14boEdMnl"
   },
   "outputs": [],
   "source": [
    "train_df.columns = train_df.columns.get_level_values(1)\n",
    "test_df.columns = test_df.columns.get_level_values(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EeUTxDaXdRDk"
   },
   "outputs": [],
   "source": [
    "train_df.reset_index(inplace=True)\n",
    "test_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3cCfuGt12emw"
   },
   "outputs": [],
   "source": [
    "def correct_dcm(dcm):\n",
    "    x = dcm.pixel_array + 1000\n",
    "    px_mode = 4096\n",
    "    x[x>=px_mode] = x[x>=px_mode] - px_mode\n",
    "    dcm.PixelData = x.tobytes()\n",
    "    dcm.RescaleIntercept = -1000\n",
    "\n",
    "def window_image(dcm, window_center, window_width):\n",
    "    \n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    \n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "\n",
    "    return img\n",
    "def bsb_window(dcm):\n",
    "    brain_img = window_image(dcm, 40, 80)\n",
    "    subdural_img = window_image(dcm, 80, 200)\n",
    "    soft_img = window_image(dcm, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
    "\n",
    "    return bsb_img\n",
    "# dicom = pydicom.dcmread(train_images_dir + 'ID_5c8b5d701' + '.dcm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UVadnyLPJqUu"
   },
   "outputs": [],
   "source": [
    "def window_with_correction(dcm, window_center, window_width):\n",
    "    if (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100):\n",
    "        correct_dcm(dcm)\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "\n",
    "def window_without_correction(dcm, window_center, window_width):\n",
    "    img = dcm.pixel_array * dcm.RescaleSlope + dcm.RescaleIntercept\n",
    "    img_min = window_center - window_width // 2\n",
    "    img_max = window_center + window_width // 2\n",
    "    img = np.clip(img, img_min, img_max)\n",
    "    return img\n",
    "def window_testing(img, window):\n",
    "    brain_img = window(img, 40, 80)\n",
    "    subdural_img = window(img, 80, 200)\n",
    "    soft_img = window(img, 40, 380)\n",
    "    \n",
    "    brain_img = (brain_img - 0) / 80\n",
    "    subdural_img = (subdural_img - (-20)) / 200\n",
    "    soft_img = (soft_img - (-150)) / 380\n",
    "    bsb_img = np.array([brain_img, subdural_img, soft_img]).transpose(1,2,0)\n",
    "\n",
    "    return bsb_img\n",
    "\n",
    "# # example of a \"bad data point\" (i.e. (dcm.BitsStored == 12) and (dcm.PixelRepresentation == 0) and (int(dcm.RescaleIntercept) > -100) == True)\n",
    "# dicom = pydicom.dcmread(train_images_dir + \"ID_036db39b7\" + \".dcm\")\n",
    "\n",
    "# fig, ax = plt.subplots(1, 2)\n",
    "# fig, ax = plt.subplots(1, 2)\n",
    "\n",
    "# ax[0].imshow(window_testing(dicom, window_without_correction), cmap=plt.cm.bone);\n",
    "# ax[0].set_title(\"original\")\n",
    "# ax[1].imshow(window_testing(dicom, window_with_correction), cmap=plt.cm.bone);\n",
    "# ax[1].set_title(\"corrected\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X8xDzVam18X6"
   },
   "outputs": [],
   "source": [
    "def _read(path, desired_size):\n",
    "    \"\"\"Will be used in DataGenerator\"\"\"\n",
    "    \n",
    "    dcm = pydicom.dcmread(path)\n",
    "    \n",
    "    try:\n",
    "        img = bsb_window(dcm)\n",
    "    except:\n",
    "        img = np.zeros(desired_size)\n",
    "    \n",
    "    \n",
    "    img = cv2.resize(img, desired_size[:2], interpolation=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WtI9iEDt2g0g"
   },
   "outputs": [],
   "source": [
    "train_images = [f for f in listdir(train_images_dir) if isfile(join(train_images_dir, f))]\n",
    "test_images = [f for f in listdir(test_images_dir) if isfile(join(test_images_dir, f))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VJ5srOrF2uwn"
   },
   "outputs": [],
   "source": [
    "train_images_ID = [s.strip('.dcm') for s in train_images]\n",
    "test_images_ID = [s.strip('.dcm') for s in test_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3vETOe4S2wWo"
   },
   "outputs": [],
   "source": [
    "p_train = []\n",
    "for num in range(70616):\n",
    "    #print(num)\n",
    "    dicom = _read(train_images_dir+train_images[num], (128, 128)) \n",
    "    p_train.append(dicom)\n",
    "#np.save('train1',p_train)\n",
    "\n",
    "p_train = []\n",
    "for num in range(70616,len(train_images)):\n",
    "    #print(num)\n",
    "    dicom = _read(train_images_dir+train_images[num], (128, 128)) \n",
    "    p_train.append(dicom)\n",
    "#np.save('train2',p_train)\n",
    "\n",
    "\n",
    "p_test = []\n",
    "for num2 in range(len(test_images)):\n",
    "    #print(num2)\n",
    "    dicom = _read(test_images_dir+test_images[num2], (128, 128)) \n",
    "    p_test.append(dicom)\n",
    "np.save('test',p_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "I-4oMLtS2wZM"
   },
   "outputs": [],
   "source": [
    "# tt = np.load('train1.npy')\n",
    "# pp = np.load('train2.npy')\n",
    "# test = np.load('test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# p_train = []\n",
    "# for item in tt:\n",
    "#     p_train.append(item)\n",
    "# for item in pp:\n",
    "#     p_train.append(item)\n",
    "# p_test = []\n",
    "# for item in test:\n",
    "#     p_test.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6ymAXB5dFF3j"
   },
   "outputs": [],
   "source": [
    "l_train =  list(zip(train_images_ID, p_train))\n",
    "train_pixel = pd.DataFrame(l_train, columns = ['ID_Image' , 'pixel']) \n",
    "l_test =  list(zip(test_images_ID, p_test))\n",
    "test_pixel = pd.DataFrame(l_test, columns = ['ID_Image' , 'pixel']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LJDuVOeHPDa8"
   },
   "outputs": [],
   "source": [
    "train_merge = train_df.merge(train_pixel, left_on='ID_Image', right_on='ID_Image', how='inner')\n",
    "test_merge = test_df.merge(test_pixel, left_on='ID_Image', right_on='ID_Image', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tu3RMGQU2Vky"
   },
   "outputs": [],
   "source": [
    "train_X = train_merge.drop(columns = ['ID_Image','any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural'])\n",
    "train_X = np.stack(train_X['pixel'].tolist(),axis=0)\n",
    "train_Y = np.array(train_merge[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']])\n",
    "test_X = test_merge.drop(columns = ['ID_Image','any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural'])\n",
    "test_X = np.stack(test_X['pixel'].tolist(),axis=0)\n",
    "test_Y = np.array(test_merge[['any', 'epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCT_AWcvP_QI"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1eAgLhb2MlJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqJolvzJn1v2"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def weighted_log_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Can be used as the loss function in model.compile()\n",
    "    ---------------------------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    class_weights = np.array([2., 1., 1., 1., 1., 1.])\n",
    "    \n",
    "    eps = K.epsilon()\n",
    "    \n",
    "    y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "\n",
    "    out = -(         y_true  * K.log(      y_pred) * class_weights\n",
    "            + (1.0 - y_true) * K.log(1.0 - y_pred) * class_weights)\n",
    "    \n",
    "    return K.mean(out, axis=-1)\n",
    "def _normalized_weighted_average(arr, weights=None):\n",
    "    \"\"\"\n",
    "    A simple Keras implementation that mimics that of \n",
    "    numpy.average(), specifically for this competition\n",
    "    \"\"\"\n",
    "    \n",
    "    if weights is not None:\n",
    "        scl = K.sum(weights)\n",
    "        weights = K.expand_dims(weights, axis=1)\n",
    "        return K.sum(K.dot(arr, weights), axis=1) / scl\n",
    "    return K.mean(arr, axis=1)\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Will be used as the metric in model.compile()\n",
    "    ---------------------------------------------\n",
    "    \n",
    "    Similar to the custom loss function 'weighted_log_loss()' above\n",
    "    but with normalized weights, which should be very similar \n",
    "    to the official competition metric:\n",
    "        https://www.kaggle.com/kambarakun/lb-probe-weights-n-of-positives-scoring\n",
    "    and hence:\n",
    "        sklearn.metrics.log_loss with sample weights\n",
    "    \"\"\"\n",
    "    \n",
    "    class_weights = K.variable([2., 1., 1., 1., 1., 1.])\n",
    "    \n",
    "    eps = K.epsilon()\n",
    "    \n",
    "    y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "\n",
    "    loss = -(        y_true  * K.log(      y_pred)\n",
    "            + (1.0 - y_true) * K.log(1.0 - y_pred))\n",
    "    \n",
    "    loss_samples = _normalized_weighted_average(loss, class_weights)\n",
    "    \n",
    "    return K.mean(loss_samples)\n",
    "\n",
    "def weighted_log_loss_metric(trues, preds):\n",
    "    \"\"\"\n",
    "    Will be used to calculate the log loss \n",
    "    of the validation set in PredictionCheckpoint()\n",
    "    ------------------------------------------\n",
    "    \"\"\"\n",
    "    class_weights = [2., 1., 1., 1., 1., 1.]\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    preds = np.clip(preds, epsilon, 1-epsilon)\n",
    "    loss = trues * np.log(preds) + (1 - trues) * np.log(1 - preds)\n",
    "    loss_samples = np.average(loss, axis=1, weights=class_weights)\n",
    "\n",
    "    return - loss_samples.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1575168497394,
     "user": {
      "displayName": "Danyang Zhang",
      "photoUrl": "",
      "userId": "02292902200231484269"
     },
     "user_tz": 360
    },
    "id": "seNXEfOUQEyJ",
    "outputId": "86da6b8a-a4ba-4b4d-c71f-c374aef7ae17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sichun1247/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/sichun1247/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sichun1247/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               4194432   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 4,288,454\n",
      "Trainable params: 4,288,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Sequential()\n",
    "# Convolutional Layer 1:\n",
    "cnn_model.add(Conv2D(32, kernel_size = (3, 3), activation = 'linear', input_shape = (128, 128, 3), padding = 'same'))\n",
    "cnn_model.add(LeakyReLU(alpha = 0.1))\n",
    "# Max Pooling Layer 1:\n",
    "cnn_model.add(MaxPooling2D((2, 2)))\n",
    "# Convolutional Layer 2:\n",
    "cnn_model.add(Conv2D(64, kernel_size = (3, 3), activation = 'linear', padding = 'same'))\n",
    "cnn_model.add(LeakyReLU(alpha = 0.1))\n",
    "# Max Pooling Layer 2:\n",
    "cnn_model.add(MaxPooling2D((2, 2)))\n",
    "# Convolutional Layer 3:\n",
    "cnn_model.add(Conv2D(128, kernel_size = (3, 3), activation = 'linear', padding = 'same'))\n",
    "cnn_model.add(LeakyReLU(alpha = 0.1))\n",
    "# Max Pooling Layer 3:\n",
    "cnn_model.add(MaxPooling2D((2, 2)))\n",
    "# Flatten Preprocessing:\n",
    "cnn_model.add(Flatten())\n",
    "# Fully Connected Layer (Dense Layer):\n",
    "cnn_model.add(Dense(128, activation = 'linear'))\n",
    "cnn_model.add(LeakyReLU(alpha = 0.1))\n",
    "# Output Layer (Dense Layer):\n",
    "cnn_model.add(Dense(6, activation = 'sigmoid')) # choose one of them? # softmax\n",
    "\n",
    "# Compile CNN model\n",
    "cnn_model.compile(loss = \"binary_crossentropy\", optimizer = keras.optimizers.Adam(), metrics = [weighted_loss])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, list_IDs, labels=None, batch_size=1, img_size=(512, 512, 1), \n",
    "                 img_dir=train_images_dir, *args, **kwargs):\n",
    "\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_dir = img_dir\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indices]\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            X, Y = self.__data_generation(list_IDs_temp)\n",
    "            return X, Y\n",
    "        else:\n",
    "            X = self.__data_generation(list_IDs_temp)\n",
    "            return X\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        \n",
    "        if self.labels is not None: # for training phase we undersample and shuffle\n",
    "            # keep probability of any=0 and any=1\n",
    "            keep_prob = self.labels.iloc[:, 0].map({0: 0.35, 1: 0.5})\n",
    "            keep = (keep_prob > np.random.rand(len(keep_prob)))\n",
    "            self.indices = np.arange(len(self.list_IDs))[keep]\n",
    "            np.random.shuffle(self.indices)\n",
    "        else:\n",
    "            self.indices = np.arange(len(self.list_IDs))\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size, *self.img_size))\n",
    "        \n",
    "        if self.labels is not None: # training phase\n",
    "            Y = np.empty((self.batch_size, 6), dtype=np.float32)\n",
    "        \n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                #X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "                X[i,] = [x[1] for x in l_train if x[0] == ID][0]\n",
    "                Y[i,] = self.labels.loc[ID].values\n",
    "        \n",
    "            return X, Y\n",
    "        \n",
    "        else: # test phase\n",
    "            if self.img_dir == '/home/sichun1247/Sampled_Test/':\n",
    "                for i, ID in enumerate(list_IDs_temp):\n",
    "                    #X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "                    X[i,] = [x[1] for x in l_test if x[0] == ID][0]\n",
    "            else:\n",
    "                for i, ID in enumerate(list_IDs_temp):\n",
    "                    #X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "                    X[i,] = [x[1] for x in l_train if x[0] == ID][0]\n",
    "            \n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 485121,
     "status": "ok",
     "timestamp": 1575168984079,
     "user": {
      "displayName": "Danyang Zhang",
      "photoUrl": "",
      "userId": "02292902200231484269"
     },
     "user_tz": 360
    },
    "id": "uvm85KIdQJTf",
    "outputId": "63823978-95dd-40c0-eceb-e0c711172905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sichun1247/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 123596 samples, validate on 18648 samples\n",
      "Epoch 1/30\n",
      "123596/123596 [==============================] - 2446s 20ms/step - loss: 0.2806 - weighted_loss: 0.2986 - val_loss: 0.3701 - val_weighted_loss: 0.3368\n",
      "Epoch 2/30\n",
      "123596/123596 [==============================] - 2443s 20ms/step - loss: 0.2183 - weighted_loss: 0.2320 - val_loss: 0.3950 - val_weighted_loss: 0.3840\n",
      "Epoch 3/30\n",
      " 63744/123596 [==============>...............] - ETA: 18:46 - loss: 0.1733 - weighted_loss: 0.1855"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "#datagen = ImageDataGenerator(\n",
    "#    rotation_range = 30,\n",
    "#    horizontal_flip = True)\n",
    "#datagen.fit(train_X)\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_weighted_loss', patience=2)\n",
    "\n",
    "# Fits the model on batches with real-time data augmentation:\n",
    "cnn_model_process = cnn_model.fit_generator(datagen.flow(train_X, train_Y, batch_size = 32),\n",
    "                                                            steps_per_epoch = len(train_X) / 32, \n",
    "                                                            epochs = 20, verbose = 1, \n",
    "                                                            validation_data = (test_X, test_Y))\n",
    "\n",
    "#cnn_model_process = cnn_model.fit(train_X, train_Y, batch_size = 32, epochs = 30, verbose = 1, \n",
    "#                                  validation_data = (test_X, test_Y))\n",
    "\n",
    "self.model.fit_generator(\n",
    "    DataGenerator(\n",
    "        train_df.index, \n",
    "        train_df, \n",
    "        self.batch_size, \n",
    "        self.input_dims, \n",
    "        train_images_dir\n",
    "    ),\n",
    "    epochs=self.num_epochs,\n",
    "    verbose=self.verbose,\n",
    "    use_multiprocessing=True,\n",
    "    workers=4,\n",
    "    callbacks=[pred_history, scheduler]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiXHcUtpY54X"
   },
   "outputs": [],
   "source": [
    "# cnn_model_process.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1575169131679,
     "user": {
      "displayName": "Danyang Zhang",
      "photoUrl": "",
      "userId": "02292902200231484269"
     },
     "user_tz": 360
    },
    "id": "hrAI_L1VQJZG",
    "outputId": "507393c5-cb62-49e4-d244-10838e3658d1"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(cnn_model_process.history['weighted_loss'], marker = 'o', label='train')\n",
    "pyplot.plot(cnn_model_process.history['val_weighted_loss'], marker = 'o', label='validation')\n",
    "pyplot.xlabel('Epochs', fontsize = 14)\n",
    "pyplot.ylabel('Weighted Loss',fontsize = 14)\n",
    "pyplot.title('CNN Weighted Loss Trainig VS Testing', fontsize = 14)\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qeNTyF4l6IZs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               4194432   \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 128)               512       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 4,288,966\n",
      "Trainable params: 4,288,710\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cnn_model_dropout = Sequential()\n",
    "# Convolutional Layer 1:\n",
    "cnn_model_dropout.add(Conv2D(32, kernel_size = (3, 3), activation = 'linear', input_shape = (128, 128, 3), padding = 'same'))\n",
    "cnn_model_dropout.add(LeakyReLU(alpha = 0.1))\n",
    "# Max Pooling Layer 1:\n",
    "cnn_model_dropout.add(MaxPooling2D((2, 2)))\n",
    "# Convolutional Layer 2:\n",
    "cnn_model_dropout.add(Conv2D(64, kernel_size = (3, 3), activation = 'linear', padding = 'same'))\n",
    "cnn_model_dropout.add(LeakyReLU(alpha = 0.1))\n",
    "# Max Pooling Layer 2:\n",
    "cnn_model_dropout.add(MaxPooling2D((2, 2)))\n",
    "# Convolutional Layer 3:\n",
    "cnn_model_dropout.add(Conv2D(128, kernel_size = (3, 3), activation = 'linear', padding = 'same'))\n",
    "cnn_model_dropout.add(LeakyReLU(alpha = 0.1))\n",
    "# Max Pooling Layer 3:\n",
    "cnn_model_dropout.add(MaxPooling2D((2, 2)))\n",
    "# Flatten Preprocessing:\n",
    "cnn_model_dropout.add(Flatten())\n",
    "# Fully Connected Layer (Dense Layer):\n",
    "cnn_model_dropout.add(Dense(128, activation = 'linear'))\n",
    "cnn_model_dropout.add(BatchNormalization())\n",
    "cnn_model_dropout.add(LeakyReLU(alpha = 0.1))\n",
    "\n",
    "# Dropout Layer 4:\n",
    "cnn_model_dropout.add(Dropout(0.3))\n",
    "# Output Layer (Dense Layer):\n",
    "cnn_model_dropout.add(Dense(6, activation = 'sigmoid'))\n",
    "\n",
    "# Compile CNN model\n",
    "cnn_model_dropout.compile(loss = \"binary_crossentropy\", optimizer = keras.optimizers.Adam(), metrics = [weighted_loss])\n",
    "cnn_model_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bq2Yac_N6V6I"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 123596 samples, validate on 18648 samples\n",
      "Epoch 1/20\n",
      "123596/123596 [==============================] - 2430s 20ms/step - loss: 0.3345 - weighted_loss: 0.3569 - val_loss: 0.6220 - val_weighted_loss: 0.6270\n",
      "Epoch 2/20\n",
      "123596/123596 [==============================] - 2419s 20ms/step - loss: 0.2834 - weighted_loss: 0.3014 - val_loss: 0.5618 - val_weighted_loss: 0.5683\n",
      "Epoch 3/20\n",
      "123596/123596 [==============================] - 2421s 20ms/step - loss: 0.2602 - weighted_loss: 0.2763 - val_loss: 0.4907 - val_weighted_loss: 0.4701\n",
      "Epoch 4/20\n",
      "123596/123596 [==============================] - 2436s 20ms/step - loss: 0.2432 - weighted_loss: 0.2584 - val_loss: 0.3978 - val_weighted_loss: 0.3672\n",
      "Epoch 5/20\n",
      "123596/123596 [==============================] - 2423s 20ms/step - loss: 0.2287 - weighted_loss: 0.2429 - val_loss: 0.3800 - val_weighted_loss: 0.3538\n",
      "Epoch 6/20\n",
      "123596/123596 [==============================] - 2417s 20ms/step - loss: 0.2150 - weighted_loss: 0.2286 - val_loss: 0.3719 - val_weighted_loss: 0.3481\n",
      "Epoch 7/20\n",
      "123596/123596 [==============================] - 2408s 19ms/step - loss: 0.2017 - weighted_loss: 0.2148 - val_loss: 0.6133 - val_weighted_loss: 0.6412\n",
      "Epoch 8/20\n",
      "123596/123596 [==============================] - 2418s 20ms/step - loss: 0.1889 - weighted_loss: 0.2012 - val_loss: 0.4692 - val_weighted_loss: 0.4655\n",
      "Epoch 9/20\n",
      "123596/123596 [==============================] - 2428s 20ms/step - loss: 0.1765 - weighted_loss: 0.1885 - val_loss: 0.3646 - val_weighted_loss: 0.3419\n",
      "Epoch 10/20\n",
      "123596/123596 [==============================] - 2416s 20ms/step - loss: 0.1651 - weighted_loss: 0.1765 - val_loss: 0.4021 - val_weighted_loss: 0.3859\n",
      "Epoch 11/20\n",
      "123596/123596 [==============================] - 2411s 20ms/step - loss: 0.1548 - weighted_loss: 0.1659 - val_loss: 0.3566 - val_weighted_loss: 0.3322\n",
      "Epoch 12/20\n",
      "123596/123596 [==============================] - 2402s 19ms/step - loss: 0.1438 - weighted_loss: 0.1541 - val_loss: 0.4103 - val_weighted_loss: 0.3969\n",
      "Epoch 13/20\n",
      " 51712/123596 [===========>..................] - ETA: 22:16 - loss: 0.1303 - weighted_loss: 0.1398"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-257ff7952cab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcnn_model_dropout_process\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn_model_dropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_Y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Fits the model on batches with real-time data augmentation:\n",
    "# cnn_model_dropout_process = cnn_model_dropout.fit_generator(datagen.flow(train_X, train_Y, batch_size = 32),\n",
    "#                                                             steps_per_epoch = len(train_X) / 32, \n",
    "#                                                             epochs = 20, verbose = 1, \n",
    "#                                                             validation_data = (test_X, test_Y))\n",
    "\n",
    "cnn_model_dropout_process = cnn_model_dropout.fit(train_X, train_Y, batch_size = 32, epochs = 20, verbose = 1, \n",
    "                                                  validation_data = (test_X, test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mx5jHULo1djt"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "axes[0].plot(range(1, len(cnn_model_process.history['loss']) + 1), cnn_model_process.history['loss'], linestyle = 'solid', marker = 'o', color = 'crimson', label = 'Training Loss')\n",
    "axes[0].plot(range(1, len(cnn_model_process.history['val_loss']) + 1), cnn_model_process.history['val_loss'], linestyle = 'solid', marker = 'o', color = 'dodgerblue', label = 'Testing Loss')\n",
    "axes[0].set_xlabel('Epochs', fontsize = 14)\n",
    "axes[0].set_ylabel('Loss',fontsize = 14)\n",
    "axes[0].set_title('CNN Loss Trainig VS Testing', fontsize = 14)\n",
    "axes[0].legend(loc = 'best')\n",
    "axes[1].plot(range(1, len(cnn_model_process.history['weighted_loss']) + 1), cnn_model_process.history['weighted_loss'], linestyle = 'solid', marker = 'o', color = 'crimson', label = 'Training Weighted Loss')\n",
    "axes[1].plot(range(1, len(cnn_model_process.history['val_weighted_loss']) + 1), cnn_model_process.history['val_weighted_loss'], linestyle = 'solid', marker = 'o', color = 'dodgerblue', label = 'Testing Weighted Loss')\n",
    "axes[1].set_xlabel('Epochs', fontsize = 14)\n",
    "axes[1].set_ylabel('Weighted Loss',fontsize = 14)\n",
    "axes[1].set_title('CNN Weighted Loss Trainig VS Testing', fontsize = 14)\n",
    "axes[1].legend(loc = 'best')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "axes[0].plot(range(1, len(cnn_model_dropout_process.history['loss']) + 1), cnn_model_dropout_process.history['loss'], linestyle = 'solid', marker = 'o', color = 'crimson', label = 'Training Accuracy')\n",
    "axes[0].plot(range(1, len(cnn_model_dropout_process.history['val_loss']) + 1), cnn_model_dropout_process.history['val_loss'], linestyle = 'solid', marker = 'o', color = 'dodgerblue', label = 'Testing Accuracy')\n",
    "axes[0].set_xlabel('Epochs', fontsize = 14)\n",
    "axes[0].set_ylabel('Loss',fontsize = 14)\n",
    "axes[0].set_title('CNN Dropout Loss Trainig VS Testing', fontsize = 14)\n",
    "axes[0].legend(loc = 'best')\n",
    "axes[1].plot(range(1, len(cnn_model_dropout_process.history['weighted_loss']) + 1), cnn_model_dropout_process.history['weighted_loss'], linestyle = 'solid', marker = 'o', color = 'crimson', label = 'Training Loss')\n",
    "axes[1].plot(range(1, len(cnn_model_dropout_process.history['val_weighted_loss']) + 1), cnn_model_dropout_process.history['val_weighted_loss'], linestyle = 'solid', marker = 'o', color = 'dodgerblue', label = 'Testing Loss')\n",
    "axes[1].set_xlabel('Epochs', fontsize = 14)\n",
    "axes[1].set_ylabel('Weighted Loss',fontsize = 14)\n",
    "axes[1].set_title('CNN Dropout Weighted Loss Trainig VS Testing', fontsize = 14)\n",
    "axes[1].legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmyLU9ky6kHN"
   },
   "outputs": [],
   "source": [
    "# class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "#     def __init__(self, list_IDs, labels=None, batch_size=1, img_size=(512, 512, 1), \n",
    "#                  img_dir=train_images_dir, *args, **kwargs):\n",
    "\n",
    "#         self.list_IDs = list_IDs\n",
    "#         self.labels = labels\n",
    "#         self.batch_size = batch_size\n",
    "#         self.img_size = img_size\n",
    "#         self.img_dir = img_dir\n",
    "#         self.on_epoch_end()\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return int(ceil(len(self.indices) / self.batch_size))\n",
    "#     def __getitem__(self, index):\n",
    "#         indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "#         list_IDs_temp = [self.list_IDs[k] for k in indices]\n",
    "        \n",
    "#         if self.labels is not None:\n",
    "#             X, Y = self.__data_generation(list_IDs_temp)\n",
    "#             return X, Y\n",
    "#         else:\n",
    "#             X = self.__data_generation(list_IDs_temp)\n",
    "#             return X\n",
    "#     def on_epoch_end(self):\n",
    "        \n",
    "        \n",
    "#         if self.labels is not None: # for training phase we undersample and shuffle\n",
    "#             # keep probability of any=0 and any=1\n",
    "#             keep_prob = self.labels.iloc[:, 0].map({0: 0.35, 1: 0.5})\n",
    "#             keep = (keep_prob > np.random.rand(len(keep_prob)))\n",
    "#             self.indices = np.arange(len(self.list_IDs))[keep]\n",
    "#             np.random.shuffle(self.indices)\n",
    "#         else:\n",
    "#             self.indices = np.arange(len(self.list_IDs))\n",
    "#     def __data_generation(self, list_IDs_temp):\n",
    "#         X = np.empty((self.batch_size, *self.img_size))\n",
    "        \n",
    "#         if self.labels is not None: # training phase\n",
    "#             Y = np.empty((self.batch_size, 6), dtype=np.float32)\n",
    "        \n",
    "#             for i, ID in enumerate(list_IDs_temp):\n",
    "#                 X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "#                 Y[i,] = self.labels.loc[ID].values\n",
    "        \n",
    "#             return X, Y\n",
    "        \n",
    "#         else: # test phase\n",
    "#             for i, ID in enumerate(list_IDs_temp):\n",
    "#                 X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "            \n",
    "#             return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIU5ggZk-2-S"
   },
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "\n",
    "# def weighted_log_loss(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Can be used as the loss function in model.compile()\n",
    "#     ---------------------------------------------------\n",
    "#     \"\"\"\n",
    "    \n",
    "#     class_weights = np.array([2., 1., 1., 1., 1., 1.])\n",
    "    \n",
    "#     eps = K.epsilon()\n",
    "    \n",
    "#     y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "\n",
    "#     out = -(         y_true  * K.log(      y_pred) * class_weights\n",
    "#             + (1.0 - y_true) * K.log(1.0 - y_pred) * class_weights)\n",
    "    \n",
    "#     return K.mean(out, axis=-1)\n",
    "# def _normalized_weighted_average(arr, weights=None):\n",
    "#     \"\"\"\n",
    "#     A simple Keras implementation that mimics that of \n",
    "#     numpy.average(), specifically for this competition\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if weights is not None:\n",
    "#         scl = K.sum(weights)\n",
    "#         weights = K.expand_dims(weights, axis=1)\n",
    "#         return K.sum(K.dot(arr, weights), axis=1) / scl\n",
    "#     return K.mean(arr, axis=1)\n",
    "\n",
    "# def weighted_loss(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Will be used as the metric in model.compile()\n",
    "#     ---------------------------------------------\n",
    "    \n",
    "#     Similar to the custom loss function 'weighted_log_loss()' above\n",
    "#     but with normalized weights, which should be very similar \n",
    "#     to the official competition metric:\n",
    "#         https://www.kaggle.com/kambarakun/lb-probe-weights-n-of-positives-scoring\n",
    "#     and hence:\n",
    "#         sklearn.metrics.log_loss with sample weights\n",
    "#     \"\"\"\n",
    "    \n",
    "#     class_weights = K.variable([2., 1., 1., 1., 1., 1.])\n",
    "    \n",
    "#     eps = K.epsilon()\n",
    "    \n",
    "#     y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "\n",
    "#     loss = -(        y_true  * K.log(      y_pred)\n",
    "#             + (1.0 - y_true) * K.log(1.0 - y_pred))\n",
    "    \n",
    "#     loss_samples = _normalized_weighted_average(loss, class_weights)\n",
    "    \n",
    "#     return K.mean(loss_samples)\n",
    "\n",
    "# def weighted_log_loss_metric(trues, preds):\n",
    "#     \"\"\"\n",
    "#     Will be used to calculate the log loss \n",
    "#     of the validation set in PredictionCheckpoint()\n",
    "#     ------------------------------------------\n",
    "#     \"\"\"\n",
    "#     class_weights = [2., 1., 1., 1., 1., 1.]\n",
    "    \n",
    "#     epsilon = 1e-7\n",
    "    \n",
    "#     preds = np.clip(preds, epsilon, 1-epsilon)\n",
    "#     loss = trues * np.log(preds) + (1 - trues) * np.log(1 - preds)\n",
    "#     loss_samples = np.average(loss, axis=1, weights=class_weights)\n",
    "\n",
    "#     return - loss_samples.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXlQlmn-_Hk-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "window1.ipynb",
   "provenance": [
    {
     "file_id": "1HCkB7y1DU3rsQENTrbO0rppc1vUuxqyi",
     "timestamp": 1574733380642
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
