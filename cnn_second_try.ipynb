{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEUHhZknbL5a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sichun1247/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import os\n",
    "from os import listdir\n",
    "\n",
    "from os.path import isfile, join\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from datetime import datetime\n",
    "\n",
    "from math import ceil, floor, log\n",
    "import cv2\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "\n",
    "import sys\n",
    "\n",
    "# from keras_applications.resnet import ResNet50\n",
    "from keras_applications.inception_v3 import InceptionV3\n",
    "\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "test_images_dir = '/home/sichun1247/Sampled_Test/'\n",
    "train_images_dir = '/home/sichun1247/Sampled_Train/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jp_zYTWNbbc9"
   },
   "outputs": [],
   "source": [
    "def read_testset(filename=\"/home/sichun1247/test_sample.csv\"):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    \n",
    "    df = df.loc[:, [\"Label\", \"Type\", \"ID_Image\"]]\n",
    "    df = df.set_index(['ID_Image', 'Type']).unstack(level=-1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def read_trainset(filename=\"/home/sichun1247/train_sample.csv\"):\n",
    "    df = pd.read_csv(filename)\n",
    "\n",
    "    \n",
    "    \n",
    "    df = df.loc[:, [\"Label\", \"Type\", \"ID_Image\"]]\n",
    "    df = df.set_index(['ID_Image', 'Type']).unstack(level=-1)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = read_trainset()   \n",
    "test_df = read_testset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z4K14boEdMnl"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "dirct = os.listdir('/home/sichun1247/Sampled_Train')\n",
    "train_ID = []\n",
    "for f_name in dirct:\n",
    "    if f_name.endswith('.dcm'):\n",
    "        f, _ = f_name.split('.')\n",
    "        train_ID.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EeUTxDaXdRDk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "dirct = os.listdir('/home/sichun1247/Sampled_Test')\n",
    "test_ID = []\n",
    "for f_name in dirct:\n",
    "    if f_name.endswith('.dcm'):\n",
    "        f, _ = f_name.split('.')\n",
    "        test_ID.append(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nSK4leCc1ed7"
   },
   "outputs": [],
   "source": [
    "tt = np.load('train1.npy')\n",
    "pp = np.load('train2.npy')\n",
    "p_train = []\n",
    "for item in tt:\n",
    "    p_train.append(item)\n",
    "for item in pp:\n",
    "    p_train.append(item)\n",
    "test = np.load('test.npy')\n",
    "p_test = []\n",
    "for item in test:\n",
    "    p_test.append(item)\n",
    "l_train =  list(zip(train_ID, p_train))\n",
    "l_test =  list(zip(test_ID, p_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1eAgLhb2MlJ"
   },
   "outputs": [],
   "source": [
    "class PredictionCheckpoint(keras.callbacks.Callback):\n",
    "    \n",
    "    def __init__(self, test_df, valid_df, \n",
    "                 test_images_dir=test_images_dir, \n",
    "                 valid_images_dir=train_images_dir, \n",
    "                 batch_size=32, input_size=(224, 224, 3)):\n",
    "        \n",
    "        self.test_df = test_df\n",
    "        self.valid_df = valid_df\n",
    "        self.test_images_dir = test_images_dir\n",
    "        self.valid_images_dir = valid_images_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.input_size = input_size\n",
    "        \n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.test_predictions = []\n",
    "        self.valid_predictions = []\n",
    "        \n",
    "    def on_epoch_end(self,batch, logs={}):\n",
    "        self.test_predictions.append(\n",
    "            self.model.predict_generator(\n",
    "                DataGenerator(self.test_df.index, None, self.batch_size, self.input_size, self.test_images_dir), verbose=2)[:len(self.test_df)])\n",
    "        \n",
    "        # Commented out to save time\n",
    "        self.valid_predictions.append(\n",
    "             self.model.predict_generator(\n",
    "                 DataGenerator(self.valid_df.index, None, self.batch_size, self.input_size, self.valid_images_dir), verbose=2)[:len(self.valid_df)])\n",
    "        \n",
    "        print(\"validation loss: %.4f\" %\n",
    "              weighted_log_loss_metric(self.valid_df.values, \n",
    "                                  np.average(self.valid_predictions, axis=0, \n",
    "                                              weights=[2**i for i in range(len(self.valid_predictions))])))\n",
    "        \n",
    "        print(\"Test loss: %.4f\" %\n",
    "              weighted_log_loss_metric(self.test_df.values, \n",
    "                                  np.average(self.test_predictions, axis=0, \n",
    "                                              weights=[2**i for i in range(len(self.test_predictions))])))\n",
    "        \n",
    "        # here you could also save the predictions with np.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCT_AWcvP_QI"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Input, Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.applications.vgg16 import VGG16, preprocess_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GqJolvzJn1v2"
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "def weighted_log_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Can be used as the loss function in model.compile()\n",
    "    ---------------------------------------------------\n",
    "    \"\"\"\n",
    "    \n",
    "    class_weights = np.array([2., 1., 1., 1., 1., 1.])\n",
    "    \n",
    "    eps = K.epsilon()\n",
    "    \n",
    "    y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "\n",
    "    out = -(         y_true  * K.log(      y_pred) * class_weights\n",
    "            + (1.0 - y_true) * K.log(1.0 - y_pred) * class_weights)\n",
    "    \n",
    "    return K.mean(out, axis=-1)\n",
    "def _normalized_weighted_average(arr, weights=None):\n",
    "    \"\"\"\n",
    "    A simple Keras implementation that mimics that of \n",
    "    numpy.average(), specifically for this competition\n",
    "    \"\"\"\n",
    "    \n",
    "    if weights is not None:\n",
    "        scl = K.sum(weights)\n",
    "        weights = K.expand_dims(weights, axis=1)\n",
    "        return K.sum(K.dot(arr, weights), axis=1) / scl\n",
    "    return K.mean(arr, axis=1)\n",
    "\n",
    "def weighted_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Will be used as the metric in model.compile()\n",
    "    ---------------------------------------------\n",
    "    \n",
    "    Similar to the custom loss function 'weighted_log_loss()' above\n",
    "    but with normalized weights, which should be very similar \n",
    "    to the official competition metric:\n",
    "        https://www.kaggle.com/kambarakun/lb-probe-weights-n-of-positives-scoring\n",
    "    and hence:\n",
    "        sklearn.metrics.log_loss with sample weights\n",
    "    \"\"\"\n",
    "    \n",
    "    class_weights = K.variable([2., 1., 1., 1., 1., 1.])\n",
    "    \n",
    "    eps = K.epsilon()\n",
    "    \n",
    "    y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "\n",
    "    loss = -(        y_true  * K.log(      y_pred)\n",
    "            + (1.0 - y_true) * K.log(1.0 - y_pred))\n",
    "    \n",
    "    loss_samples = _normalized_weighted_average(loss, class_weights)\n",
    "    \n",
    "    return K.mean(loss_samples)\n",
    "\n",
    "def weighted_log_loss_metric(trues, preds):\n",
    "    \"\"\"\n",
    "    Will be used to calculate the log loss \n",
    "    of the validation set in PredictionCheckpoint()\n",
    "    ------------------------------------------\n",
    "    \"\"\"\n",
    "    class_weights = [2., 1., 1., 1., 1., 1.]\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    preds = np.clip(preds, epsilon, 1-epsilon)\n",
    "    loss = trues * np.log(preds) + (1 - trues) * np.log(1 - preds)\n",
    "    loss_samples = np.average(loss, axis=1, weights=class_weights)\n",
    "\n",
    "    return - loss_samples.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 595
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 382,
     "status": "ok",
     "timestamp": 1575168497394,
     "user": {
      "displayName": "Danyang Zhang",
      "photoUrl": "",
      "userId": "02292902200231484269"
     },
     "user_tz": 360
    },
    "id": "seNXEfOUQEyJ",
    "outputId": "86da6b8a-a4ba-4b4d-c71f-c374aef7ae17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sichun1247/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /home/sichun1247/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4070: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From /home/sichun1247/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 32)      896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 64)        18496     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 32768)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               4194432   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 6)                 774       \n",
      "=================================================================\n",
      "Total params: 4,288,454\n",
      "Trainable params: 4,288,454\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn_model = Sequential()\n",
    "# Convolutional Layer 1:\n",
    "cnn_model.add(Conv2D(32, kernel_size = (3, 3), activation = 'linear', input_shape = (128, 128, 3), padding = 'same'))\n",
    "cnn_model.add(LeakyReLU(alpha = 0.1))\n",
    "# Max Pooling Layer 1:\n",
    "cnn_model.add(MaxPooling2D((2, 2)))\n",
    "# Convolutional Layer 2:\n",
    "cnn_model.add(Conv2D(64, kernel_size = (3, 3), activation = 'linear', padding = 'same'))\n",
    "cnn_model.add(LeakyReLU(alpha = 0.1))\n",
    "# Max Pooling Layer 2:\n",
    "cnn_model.add(MaxPooling2D((2, 2)))\n",
    "# Convolutional Layer 3:\n",
    "cnn_model.add(Conv2D(128, kernel_size = (3, 3), activation = 'linear', padding = 'same'))\n",
    "cnn_model.add(LeakyReLU(alpha = 0.1))\n",
    "# Max Pooling Layer 3:\n",
    "cnn_model.add(MaxPooling2D((2, 2)))\n",
    "# Flatten Preprocessing:\n",
    "cnn_model.add(Flatten())\n",
    "# Fully Connected Layer (Dense Layer):\n",
    "cnn_model.add(Dense(128, activation = 'linear'))\n",
    "cnn_model.add(LeakyReLU(alpha = 0.1))\n",
    "# Output Layer (Dense Layer):\n",
    "cnn_model.add(Dense(6, activation = 'sigmoid')) # choose one of them? # softmax\n",
    "\n",
    "# Compile CNN model\n",
    "cnn_model.compile(loss = \"binary_crossentropy\", optimizer = keras.optimizers.Adam(), metrics = [weighted_loss])\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "    def __init__(self, list_IDs, labels=None, batch_size=1, img_size=(512, 512, 1), \n",
    "                 img_dir=train_images_dir, *args, **kwargs):\n",
    "\n",
    "        self.list_IDs = list_IDs\n",
    "        self.labels = labels\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.img_dir = img_dir\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(ceil(len(self.indices) / self.batch_size))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        list_IDs_temp = [self.list_IDs[k] for k in indices]\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            X, Y = self.__data_generation(list_IDs_temp)\n",
    "            return X, Y\n",
    "        else:\n",
    "            X = self.__data_generation(list_IDs_temp)\n",
    "            return X\n",
    "        \n",
    "    def on_epoch_end(self):\n",
    "        \n",
    "        \n",
    "        if self.labels is not None: # for training phase we undersample and shuffle\n",
    "            # keep probability of any=0 and any=1\n",
    "            keep_prob = self.labels.iloc[:, 0].map({0: 0.35, 1: 0.5})\n",
    "            keep = (keep_prob > np.random.rand(len(keep_prob)))\n",
    "            self.indices = np.arange(len(self.list_IDs))[keep]\n",
    "            np.random.shuffle(self.indices)\n",
    "        else:\n",
    "            self.indices = np.arange(len(self.list_IDs))\n",
    "\n",
    "    def __data_generation(self, list_IDs_temp):\n",
    "        X = np.empty((self.batch_size, *self.img_size))\n",
    "        \n",
    "        if self.labels is not None: # training phase\n",
    "            Y = np.empty((self.batch_size, 6), dtype=np.float32)\n",
    "        \n",
    "            for i, ID in enumerate(list_IDs_temp):\n",
    "                #X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "                X[i,] = [x[1] for x in l_train if x[0] == ID][0]\n",
    "                Y[i,] = self.labels.loc[ID].values\n",
    "        \n",
    "            return X, Y\n",
    "        \n",
    "        else: # test phase\n",
    "            if self.img_dir == '/home/sichun1247/Sampled_Test/':\n",
    "                for i, ID in enumerate(list_IDs_temp):\n",
    "                    #X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "                    X[i,] = [x[1] for x in l_test if x[0] == ID][0]\n",
    "            else:\n",
    "                for i, ID in enumerate(list_IDs_temp):\n",
    "                    #X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "                    X[i,] = [x[1] for x in l_train if x[0] == ID][0]\n",
    "            \n",
    "            return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 989
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 485121,
     "status": "ok",
     "timestamp": 1575168984079,
     "user": {
      "displayName": "Danyang Zhang",
      "photoUrl": "",
      "userId": "02292902200231484269"
     },
     "user_tz": 360
    },
    "id": "uvm85KIdQJTf",
    "outputId": "63823978-95dd-40c0-eceb-e0c711172905"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/sichun1247/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/20\n",
      "3973/3972 [==============================] - 2816s 709ms/step - loss: 0.3001 - weighted_loss: 0.3138\n",
      "validation loss: 0.2731\n",
      "Test loss: 0.2915\n",
      "Epoch 2/20\n",
      "3973/3972 [==============================] - 2809s 707ms/step - loss: 0.2194 - weighted_loss: 0.2303\n",
      "validation loss: 0.2353\n",
      "Test loss: 0.2562\n",
      "Epoch 3/20\n",
      "3973/3972 [==============================] - 2786s 701ms/step - loss: 0.1616 - weighted_loss: 0.1722\n",
      "validation loss: 0.2126\n",
      "Test loss: 0.2378\n",
      "Epoch 4/20\n",
      "3973/3972 [==============================] - 2791s 703ms/step - loss: 0.1162 - weighted_loss: 0.1257\n",
      "validation loss: 0.1982\n",
      "Test loss: 0.2247\n",
      "Epoch 5/20\n",
      "3973/3972 [==============================] - 2784s 701ms/step - loss: 0.0883 - weighted_loss: 0.0964\n",
      "validation loss: 0.1973\n",
      "Test loss: 0.2253\n",
      "Epoch 6/20\n",
      "3973/3972 [==============================] - 2791s 702ms/step - loss: 0.0659 - weighted_loss: 0.0724\n",
      "validation loss: 0.1993\n",
      "Test loss: 0.2312\n",
      "Epoch 7/20\n",
      " 293/3972 [=>............................] - ETA: 43:16 - loss: 0.0543 - weighted_loss: 0.0597"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-52233f73f98a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                                     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                                     \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpred_history\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                                 )\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[1;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1730\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1731\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1732\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1733\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    218\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                                             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m                                             reset_metrics=False)\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1512\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1514\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1515\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1516\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "#datagen = ImageDataGenerator(\n",
    "#    rotation_range = 30,\n",
    "#    horizontal_flip = True)\n",
    "#datagen.fit(train_X)\n",
    "test_df = test_df\n",
    "df = train_df\n",
    "\n",
    "#early_stopping = EarlyStopping(monitor='val_weighted_loss', patience=2)\n",
    "ss = ShuffleSplit(n_splits=10, test_size=0.1, random_state=42).split(df.index)\n",
    "train_idx, valid_idx = next(ss)\n",
    "\n",
    "pred_history = PredictionCheckpoint(test_df, df.iloc[valid_idx], input_size=(128, 128, 3))\n",
    "\n",
    "# Fits the model on batches with real-time data augmentation:\n",
    "#cnn_model_process = cnn_model.fit_generator(datagen.flow(train_X, train_Y, batch_size = 32),\n",
    "#                                                             steps_per_epoch = len(train_X) / 32, \n",
    "#                                                             epochs = 20, verbose = 1, \n",
    "#                                                             validation_data = (test_X, test_Y))\n",
    "\n",
    "#cnn_model_process = cnn_model.fit(train_X, train_Y, batch_size = 32, epochs = 30, verbose = 1, \n",
    "#                                  validation_data = (test_X, test_Y))\n",
    "\n",
    "cnn_model_process = cnn_model.fit_generator(\n",
    "                                    DataGenerator(\n",
    "                                        df.iloc[train_idx].index, \n",
    "                                        df.iloc[train_idx], \n",
    "                                        batch_size = 32, \n",
    "                                        img_size=(128, 128, 3), \n",
    "                                        img_dir=train_images_dir\n",
    "                                    ),\n",
    "                                    steps_per_epoch = len(df.iloc[train_idx]) / 32,\n",
    "                                    epochs=20,\n",
    "                                    verbose=1,\n",
    "                                    callbacks=[pred_history]\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qiXHcUtpY54X"
   },
   "outputs": [],
   "source": [
    "# cnn_model_process.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 658,
     "status": "ok",
     "timestamp": 1575169131679,
     "user": {
      "displayName": "Danyang Zhang",
      "photoUrl": "",
      "userId": "02292902200231484269"
     },
     "user_tz": 360
    },
    "id": "hrAI_L1VQJZG",
    "outputId": "507393c5-cb62-49e4-d244-10838e3658d1"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot\n",
    "\n",
    "pyplot.plot(cnn_model_process.history['weighted_loss'], marker = 'o', label='train')\n",
    "pyplot.plot(cnn_model_process.history['val_weighted_loss'], marker = 'o', label='validation')\n",
    "pyplot.xlabel('Epochs', fontsize = 14)\n",
    "pyplot.ylabel('Weighted Loss',fontsize = 14)\n",
    "pyplot.title('CNN Weighted Loss Trainig VS Testing', fontsize = 14)\n",
    "pyplot.legend()\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qeNTyF4l6IZs"
   },
   "outputs": [],
   "source": [
    "\n",
    "cnn_model_dropout = Sequential()\n",
    "# Convolutional Layer 1:\n",
    "cnn_model_dropout.add(Conv2D(32, kernel_size = (3, 3), activation = 'linear', input_shape = (128, 128, 3), padding = 'same'))\n",
    "cnn_model_dropout.add(LeakyReLU(alpha = 0.1))\n",
    "# Max Pooling Layer 1:\n",
    "cnn_model_dropout.add(MaxPooling2D((2, 2)))\n",
    "# Convolutional Layer 2:\n",
    "cnn_model_dropout.add(Conv2D(64, kernel_size = (3, 3), activation = 'linear', padding = 'same'))\n",
    "cnn_model_dropout.add(LeakyReLU(alpha = 0.1))\n",
    "# Max Pooling Layer 2:\n",
    "cnn_model_dropout.add(MaxPooling2D((2, 2)))\n",
    "# Convolutional Layer 3:\n",
    "cnn_model_dropout.add(Conv2D(128, kernel_size = (3, 3), activation = 'linear', padding = 'same'))\n",
    "cnn_model_dropout.add(LeakyReLU(alpha = 0.1))\n",
    "# Max Pooling Layer 3:\n",
    "cnn_model_dropout.add(MaxPooling2D((2, 2)))\n",
    "# Flatten Preprocessing:\n",
    "cnn_model_dropout.add(Flatten())\n",
    "# Fully Connected Layer (Dense Layer):\n",
    "cnn_model_dropout.add(Dense(128, activation = 'linear'))\n",
    "cnn_model_dropout.add(BatchNormalization())\n",
    "cnn_model_dropout.add(LeakyReLU(alpha = 0.1))\n",
    "\n",
    "# Dropout Layer 4:\n",
    "cnn_model_dropout.add(Dropout(0.3))\n",
    "# Output Layer (Dense Layer):\n",
    "cnn_model_dropout.add(Dense(6, activation = 'sigmoid'))\n",
    "\n",
    "# Compile CNN model\n",
    "cnn_model_dropout.compile(loss = \"binary_crossentropy\", optimizer = keras.optimizers.Adam(), metrics = [weighted_loss])\n",
    "cnn_model_dropout.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bq2Yac_N6V6I"
   },
   "outputs": [],
   "source": [
    "# Fits the model on batches with real-time data augmentation:\n",
    "# cnn_model_dropout_process = cnn_model_dropout.fit_generator(datagen.flow(train_X, train_Y, batch_size = 32),\n",
    "#                                                             steps_per_epoch = len(train_X) / 32, \n",
    "#                                                             epochs = 20, verbose = 1, \n",
    "#                                                             validation_data = (test_X, test_Y))\n",
    "\n",
    "cnn_model_dropout_process = cnn_model_dropout.fit(train_X, train_Y, batch_size = 32, epochs = 20, verbose = 1, \n",
    "                                                  validation_data = (test_X, test_Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mx5jHULo1djt"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "axes[0].plot(range(1, len(cnn_model_process.history['loss']) + 1), cnn_model_process.history['loss'], linestyle = 'solid', marker = 'o', color = 'crimson', label = 'Training Loss')\n",
    "axes[0].plot(range(1, len(cnn_model_process.history['val_loss']) + 1), cnn_model_process.history['val_loss'], linestyle = 'solid', marker = 'o', color = 'dodgerblue', label = 'Testing Loss')\n",
    "axes[0].set_xlabel('Epochs', fontsize = 14)\n",
    "axes[0].set_ylabel('Loss',fontsize = 14)\n",
    "axes[0].set_title('CNN Loss Trainig VS Testing', fontsize = 14)\n",
    "axes[0].legend(loc = 'best')\n",
    "axes[1].plot(range(1, len(cnn_model_process.history['weighted_loss']) + 1), cnn_model_process.history['weighted_loss'], linestyle = 'solid', marker = 'o', color = 'crimson', label = 'Training Weighted Loss')\n",
    "axes[1].plot(range(1, len(cnn_model_process.history['val_weighted_loss']) + 1), cnn_model_process.history['val_weighted_loss'], linestyle = 'solid', marker = 'o', color = 'dodgerblue', label = 'Testing Weighted Loss')\n",
    "axes[1].set_xlabel('Epochs', fontsize = 14)\n",
    "axes[1].set_ylabel('Weighted Loss',fontsize = 14)\n",
    "axes[1].set_title('CNN Weighted Loss Trainig VS Testing', fontsize = 14)\n",
    "axes[1].legend(loc = 'best')\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize = (10, 5))\n",
    "axes[0].plot(range(1, len(cnn_model_dropout_process.history['loss']) + 1), cnn_model_dropout_process.history['loss'], linestyle = 'solid', marker = 'o', color = 'crimson', label = 'Training Accuracy')\n",
    "axes[0].plot(range(1, len(cnn_model_dropout_process.history['val_loss']) + 1), cnn_model_dropout_process.history['val_loss'], linestyle = 'solid', marker = 'o', color = 'dodgerblue', label = 'Testing Accuracy')\n",
    "axes[0].set_xlabel('Epochs', fontsize = 14)\n",
    "axes[0].set_ylabel('Loss',fontsize = 14)\n",
    "axes[0].set_title('CNN Dropout Loss Trainig VS Testing', fontsize = 14)\n",
    "axes[0].legend(loc = 'best')\n",
    "axes[1].plot(range(1, len(cnn_model_dropout_process.history['weighted_loss']) + 1), cnn_model_dropout_process.history['weighted_loss'], linestyle = 'solid', marker = 'o', color = 'crimson', label = 'Training Loss')\n",
    "axes[1].plot(range(1, len(cnn_model_dropout_process.history['val_weighted_loss']) + 1), cnn_model_dropout_process.history['val_weighted_loss'], linestyle = 'solid', marker = 'o', color = 'dodgerblue', label = 'Testing Loss')\n",
    "axes[1].set_xlabel('Epochs', fontsize = 14)\n",
    "axes[1].set_ylabel('Weighted Loss',fontsize = 14)\n",
    "axes[1].set_title('CNN Dropout Weighted Loss Trainig VS Testing', fontsize = 14)\n",
    "axes[1].legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qmyLU9ky6kHN"
   },
   "outputs": [],
   "source": [
    "# class DataGenerator(keras.utils.Sequence):\n",
    "\n",
    "#     def __init__(self, list_IDs, labels=None, batch_size=1, img_size=(512, 512, 1), \n",
    "#                  img_dir=train_images_dir, *args, **kwargs):\n",
    "\n",
    "#         self.list_IDs = list_IDs\n",
    "#         self.labels = labels\n",
    "#         self.batch_size = batch_size\n",
    "#         self.img_size = img_size\n",
    "#         self.img_dir = img_dir\n",
    "#         self.on_epoch_end()\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return int(ceil(len(self.indices) / self.batch_size))\n",
    "#     def __getitem__(self, index):\n",
    "#         indices = self.indices[index*self.batch_size:(index+1)*self.batch_size]\n",
    "#         list_IDs_temp = [self.list_IDs[k] for k in indices]\n",
    "        \n",
    "#         if self.labels is not None:\n",
    "#             X, Y = self.__data_generation(list_IDs_temp)\n",
    "#             return X, Y\n",
    "#         else:\n",
    "#             X = self.__data_generation(list_IDs_temp)\n",
    "#             return X\n",
    "#     def on_epoch_end(self):\n",
    "        \n",
    "        \n",
    "#         if self.labels is not None: # for training phase we undersample and shuffle\n",
    "#             # keep probability of any=0 and any=1\n",
    "#             keep_prob = self.labels.iloc[:, 0].map({0: 0.35, 1: 0.5})\n",
    "#             keep = (keep_prob > np.random.rand(len(keep_prob)))\n",
    "#             self.indices = np.arange(len(self.list_IDs))[keep]\n",
    "#             np.random.shuffle(self.indices)\n",
    "#         else:\n",
    "#             self.indices = np.arange(len(self.list_IDs))\n",
    "#     def __data_generation(self, list_IDs_temp):\n",
    "#         X = np.empty((self.batch_size, *self.img_size))\n",
    "        \n",
    "#         if self.labels is not None: # training phase\n",
    "#             Y = np.empty((self.batch_size, 6), dtype=np.float32)\n",
    "        \n",
    "#             for i, ID in enumerate(list_IDs_temp):\n",
    "#                 X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "#                 Y[i,] = self.labels.loc[ID].values\n",
    "        \n",
    "#             return X, Y\n",
    "        \n",
    "#         else: # test phase\n",
    "#             for i, ID in enumerate(list_IDs_temp):\n",
    "#                 X[i,] = _read(self.img_dir+ID+\".dcm\", self.img_size)\n",
    "            \n",
    "#             return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LIU5ggZk-2-S"
   },
   "outputs": [],
   "source": [
    "# from keras import backend as K\n",
    "\n",
    "# def weighted_log_loss(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Can be used as the loss function in model.compile()\n",
    "#     ---------------------------------------------------\n",
    "#     \"\"\"\n",
    "    \n",
    "#     class_weights = np.array([2., 1., 1., 1., 1., 1.])\n",
    "    \n",
    "#     eps = K.epsilon()\n",
    "    \n",
    "#     y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "\n",
    "#     out = -(         y_true  * K.log(      y_pred) * class_weights\n",
    "#             + (1.0 - y_true) * K.log(1.0 - y_pred) * class_weights)\n",
    "    \n",
    "#     return K.mean(out, axis=-1)\n",
    "# def _normalized_weighted_average(arr, weights=None):\n",
    "#     \"\"\"\n",
    "#     A simple Keras implementation that mimics that of \n",
    "#     numpy.average(), specifically for this competition\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if weights is not None:\n",
    "#         scl = K.sum(weights)\n",
    "#         weights = K.expand_dims(weights, axis=1)\n",
    "#         return K.sum(K.dot(arr, weights), axis=1) / scl\n",
    "#     return K.mean(arr, axis=1)\n",
    "\n",
    "# def weighted_loss(y_true, y_pred):\n",
    "#     \"\"\"\n",
    "#     Will be used as the metric in model.compile()\n",
    "#     ---------------------------------------------\n",
    "    \n",
    "#     Similar to the custom loss function 'weighted_log_loss()' above\n",
    "#     but with normalized weights, which should be very similar \n",
    "#     to the official competition metric:\n",
    "#         https://www.kaggle.com/kambarakun/lb-probe-weights-n-of-positives-scoring\n",
    "#     and hence:\n",
    "#         sklearn.metrics.log_loss with sample weights\n",
    "#     \"\"\"\n",
    "    \n",
    "#     class_weights = K.variable([2., 1., 1., 1., 1., 1.])\n",
    "    \n",
    "#     eps = K.epsilon()\n",
    "    \n",
    "#     y_pred = K.clip(y_pred, eps, 1.0-eps)\n",
    "\n",
    "#     loss = -(        y_true  * K.log(      y_pred)\n",
    "#             + (1.0 - y_true) * K.log(1.0 - y_pred))\n",
    "    \n",
    "#     loss_samples = _normalized_weighted_average(loss, class_weights)\n",
    "    \n",
    "#     return K.mean(loss_samples)\n",
    "\n",
    "# def weighted_log_loss_metric(trues, preds):\n",
    "#     \"\"\"\n",
    "#     Will be used to calculate the log loss \n",
    "#     of the validation set in PredictionCheckpoint()\n",
    "#     ------------------------------------------\n",
    "#     \"\"\"\n",
    "#     class_weights = [2., 1., 1., 1., 1., 1.]\n",
    "    \n",
    "#     epsilon = 1e-7\n",
    "    \n",
    "#     preds = np.clip(preds, epsilon, 1-epsilon)\n",
    "#     loss = trues * np.log(preds) + (1 - trues) * np.log(1 - preds)\n",
    "#     loss_samples = np.average(loss, axis=1, weights=class_weights)\n",
    "\n",
    "#     return - loss_samples.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yXlQlmn-_Hk-"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "window1.ipynb",
   "provenance": [
    {
     "file_id": "1HCkB7y1DU3rsQENTrbO0rppc1vUuxqyi",
     "timestamp": 1574733380642
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
